{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.optimize as so\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.numpy.linalg as jnl\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import os\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# set up computational environment\n",
    "###############################################################\n",
    "\n",
    "# get path to directory containing amat from command line\n",
    "cmdlinearg = sys.argv[1]\n",
    "print('Command line argument:', cmdlinearg)\n",
    "\n",
    "# transform commandline argument to path object\n",
    "cwddir = pathlib.Path(cmdlinearg)\n",
    "print('Current working directory:', cwddir)\n",
    "\n",
    "# load compuational parameters\n",
    "L, numx, numfour, dt, numts = np.load(cwddir / 'cmpenv.npy')\n",
    "numx = int(numx)\n",
    "numfour = int(numfour)\n",
    "numts = int(numts)\n",
    "\n",
    "# load saved variable\n",
    "a0vec = np.load(cwddir / 'a0vec.npy')\n",
    "amattruevec = np.load(cwddir / 'amattruevec.npy')\n",
    "\n",
    "fourtox = np.load(cwddir / 'fourtox.npy')\n",
    "# vtoeptrue = np.load(cwddir / 'vtoeptrue.npy')\n",
    "# vxvec = np.load(cwddir / 'vxvec.npy')\n",
    "print('Computational environment loaded.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# recreate variables from loaded data\n",
    "###############################################################\n",
    "\n",
    "# real space grid points (for plotting)\n",
    "xvec = np.linspace(-L, L, numx)\n",
    "\n",
    "# number of Toeplitz elements in the Fourier representation\n",
    "numtoepelms = 2 * numfour + 1\n",
    "\n",
    "# make kinetic operator in the Fourier representation\n",
    "# (this is constant for a given system)\n",
    "kmat = np.diag(np.arange(-numfour, numfour + 1) ** 2 * np.pi ** 2 / (2 * L ** 2))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# inverse problem\n",
    "###############################################################\n",
    "\n",
    "print('Starting inverse problem.')\n",
    "\n",
    "# make |\\psi(t)|^2 training data from amattruevec\n",
    "betamatvec = []\n",
    "for thisamattrue in amattruevec:\n",
    "    tempbetamat = []\n",
    "    for thisavectrue in thisamattrue:\n",
    "        tempbetamat.append(jnp.correlate(thisavectrue, thisavectrue, 'same'))\n",
    "\n",
    "    betamatvec.append(jnp.array(tempbetamat))\n",
    "\n",
    "betamatvec = jnp.array(betamatvec) / jnp.sqrt(2 * L)\n",
    "\n",
    "print('Training data generated.')\n",
    "\n",
    "# Toeplitz indexing matrix, used for constructing Toeplitz matrix\n",
    "# from a vector setup like:\n",
    "# jnp.concatenate([jnp.flipud(row.conj()), row[1:]])\n",
    "aa = (-1) * np.arange(0, numtoepelms).reshape(numtoepelms, 1)\n",
    "bb = [np.arange(numtoepelms - 1, 2 * numtoepelms - 1)]\n",
    "toepindxmat = np.array(aa + bb)\n",
    "# print(toepindxmat.shape)\n",
    "\n",
    "# define objective function\n",
    "def ampsquaredobjective(theta):\n",
    "    # theta is a vector containing the concatenation\n",
    "    # of the real and imaginary parts of vmat\n",
    "    # its size should be 2 * numtoepelms - 1 = 4 * numfour + 1\n",
    "\n",
    "    # to use theta we need to first recombine the real\n",
    "    # and imaginary parts into a vector of complex values\n",
    "    vtoephatR = theta[:numtoepelms]\n",
    "    vtoephatI = jnp.concatenate((jnp.array([0.0]), theta[numtoepelms:]))\n",
    "    vtoephat = vtoephatR + 1j * vtoephatI\n",
    "\n",
    "    # construct vmathat from complex toeplitz vector\n",
    "    vmathat = jnp.concatenate([jnp.flipud(jnp.conj(vtoephat)), vtoephat[1:]])[toepindxmat]\n",
    "\n",
    "    # Construct Hamiltonian matrix\n",
    "    hmathat = kmat + vmathat\n",
    "\n",
    "    # eigen-decomposition of the Hamiltonian matrix\n",
    "    spchat, stthat = jnl.eigh(hmathat)\n",
    "\n",
    "    # compute propagator matrix\n",
    "    propahat = stthat @ jnp.diag(jnp.exp(-1j * spchat * dt)) @ stthat.conj().T\n",
    "\n",
    "    rtnobj = 0.0\n",
    "    for r in range(len(a0vec)):\n",
    "        thisahat = a0vec[r].copy()\n",
    "        thisbetahatmat = [jnp.correlate(thisahat, thisahat, 'same') / jnp.sqrt(2 * L)]\n",
    "\n",
    "        # propagate system starting from initial \"a\" state\n",
    "        for i in range(numts):\n",
    "            # propagate the system one time-step\n",
    "            thisahat = (propahat @ thisahat)\n",
    "            # calculate the amp^2\n",
    "            thisbetahatmat.append(jnp.correlate(thisahat, thisahat, 'same') / jnp.sqrt(2 * L))\n",
    "\n",
    "        # compute objective functions\n",
    "        tempresid = jnp.array(thisbetahatmat) - betamatvec[r]\n",
    "        thisobj = 0.5 * jnp.sum(jnp.abs(tempresid)**2)\n",
    "        rtnobj += thisobj\n",
    "\n",
    "    return rtnobj\n",
    "\n",
    "\n",
    "# true potential in the form of theta (for testing purposes)\n",
    "# thetatrue = jnp.concatenate((jnp.real(vtoeptrue), jnp.imag(vtoeptrue[1:])))\n",
    "\n",
    "# jit ampsquaredobjective\n",
    "jitampsqobject = jax.jit(ampsquaredobjective)\n",
    "# complie and test jitampsqobject\n",
    "# print(jitampsqobject(thetatrue))\n",
    "\n",
    "# initialize theta with random coefficients close to zero\n",
    "seed = 1234  # set to None for random initialization\n",
    "# thetarnd = 0.001 * np.random.default_rng(seed).normal(size=thetatrue.shape)\n",
    "thetarnd = 0.001 * np.random.default_rng(seed).normal(size=numfour)\n",
    "thetarnd = jnp.array(thetarnd)\n",
    "\n",
    "# transform randtheta theta (i.e., initvhatmat) to real space potential\n",
    "vtoepinitR = thetarnd[:numtoepelms]\n",
    "vtoepinitI = jnp.concatenate((jnp.array([0.0]), thetarnd[numtoepelms:]))\n",
    "vtoepinit = vtoepinitR + 1j * vtoepinitI\n",
    "vinitfour = np.sqrt(2 * L) * np.concatenate([np.conjugate(np.flipud(vtoepinit[1:(numfour + 1)])), vtoepinit[:(numfour + 1)]])\n",
    "vinitrec = vinitfour @ fourtox\n",
    "\n",
    "# function for generating M and P matrix (used in adjoint method)\n",
    "def mk_M_and_P(avec):\n",
    "    halflen = len(avec) // 2\n",
    "    padavec = jnp.concatenate((jnp.zeros(halflen), jnp.array(avec), jnp.zeros(halflen)))\n",
    "\n",
    "    rawmat = []\n",
    "    for j in range(2 * halflen + 1):\n",
    "        rawmat.append(padavec[2 * halflen - j:4 * halflen + 1 - j])\n",
    "\n",
    "    Mmat = jnp.conjugate(jnp.array(rawmat))\n",
    "    Pmat = jnp.flipud(jnp.array(rawmat))\n",
    "\n",
    "    return Mmat, Pmat\n",
    "\n",
    "# jit mk_M_and_P\n",
    "jit_mk_M_and_P = jax.jit(mk_M_and_P)\n",
    "\n",
    "# function for computing gradients using adjoint method\n",
    "def adjgrads(theta):\n",
    "    # to use theta we need to first recombine the real\n",
    "    # and imaginary parts into a vector of complex values\n",
    "    vtoephatR = theta[:numtoepelms]\n",
    "    vtoephatI = jnp.concatenate((jnp.array([0.0]), theta[numtoepelms:]))\n",
    "    vtoephat = vtoephatR + 1j * vtoephatI\n",
    "    # print('Shape vtoephat:', vtoephat.shape)\n",
    "\n",
    "    # construct vmathat from complex toeplitz vector\n",
    "    vmathat = jnp.concatenate([jnp.flipud(jnp.conj(vtoephat)), vtoephat[1:]])[toepindxmat]\n",
    "\n",
    "    # Construct Hamiltonian matrix\n",
    "    hmathat = kmat + vmathat\n",
    "\n",
    "    # eigen-decomposition of the Hamiltonian matrix\n",
    "    spchat, stthat = jnl.eigh(hmathat)\n",
    "\n",
    "    # compute propagator matrix\n",
    "    propahat = stthat @ jnp.diag(jnp.exp(-1j * spchat * dt)) @ stthat.conj().T\n",
    "    proplam = jnp.transpose(jnp.conjugate(propahat))\n",
    "\n",
    "    # forward propagation\n",
    "    ahatmatvec = []\n",
    "    lammatvec = []\n",
    "    for r in range(len(a0vec)):\n",
    "        # propagate system starting from initial \"a\" state\n",
    "        thisahatmat = [a0vec[r].copy()]\n",
    "        thisrhomat = [jnp.correlate(thisahatmat[0], thisahatmat[0], 'same') / jnp.sqrt(2 * L)]\n",
    "        thispartlammat = [jnp.zeros(numtoepelms, dtype=complex)]\n",
    "\n",
    "        for i in range(numts):\n",
    "            # propagate the system one time-step\n",
    "            thisahatmat.append(propahat @ thisahatmat[-1])\n",
    "\n",
    "            # calculate the amp^2\n",
    "            thisrhomat.append(jnp.correlate(thisahatmat[-1], thisahatmat[-1], 'same') / jnp.sqrt(2 * L))\n",
    "\n",
    "            # compute \\rho^r - \\beta^r\n",
    "            thiserr = thisrhomat[-1] - betamatvec[r, i+1]\n",
    "\n",
    "            # compute M and P matrix for lambda mat\n",
    "            thisMmat, thisPmat = jit_mk_M_and_P(thisahatmat[-1])\n",
    "\n",
    "            # compute part of lambda mat\n",
    "            # ( 1 / \\sqrt{2 L} ) * [ ( M^r )^\\dagger * ( \\rho^r - \\beta^r )\n",
    "            # + \\overline{( P^r )^\\dagger * ( \\rho^r - \\beta^r )} ]\n",
    "            thispartlammat.append((thisMmat.conj().T @ thiserr + (thisPmat.conj().T @ thiserr).conj()) / jnp.sqrt(2 * L))\n",
    "\n",
    "        ahatmatvec.append(jnp.array(thisahatmat))\n",
    "\n",
    "        # build lammat backwards then flip at the end\n",
    "        thislammat = [thispartlammat[-1]]\n",
    "        for i in range(2, numts + 2):\n",
    "            thislammat.append(thispartlammat[-i] + proplam @ thislammat[-1])\n",
    "\n",
    "        lammatvec.append(jnp.flipud(jnp.array(thislammat)))\n",
    "\n",
    "    # make lists into JAX array object\n",
    "    ahatmatvec = jnp.array(ahatmatvec)\n",
    "    lammatvec = jnp.array(lammatvec)\n",
    "\n",
    "\n",
    "    #######################################\n",
    "    # the remainder of this function is for computing the\n",
    "    # gradient of the exponential matrix\n",
    "    #######################################\n",
    "\n",
    "    offdiagmask = jnp.ones((numtoepelms, numtoepelms)) - jnp.eye(numtoepelms)\n",
    "    expspec = jnp.exp(-1j * dt * spchat)\n",
    "    e1, e2 = jnp.meshgrid(expspec, expspec)\n",
    "    s1, s2 = jnp.meshgrid(spchat, spchat)\n",
    "    denom = offdiagmask * (-1j * dt) * (s1 - s2) + jnp.eye(numtoepelms)\n",
    "    mask = offdiagmask * (e1 - e2)/denom + jnp.diag(expspec)\n",
    "\n",
    "    myeye = jnp.eye(numtoepelms)\n",
    "    wsR = jnp.hstack([jnp.fliplr(myeye), myeye[:,1:]]).T\n",
    "    ctrmatsR = wsR[toepindxmat]\n",
    "    prederivamatR = jnp.einsum('ij,jkm,kl->ilm', stthat.conj().T, ctrmatsR,stthat)\n",
    "    derivamatR = prederivamatR * jnp.expand_dims(mask,2)\n",
    "    alldmatreal = -1j * dt * jnp.einsum('ij,jkm,kl->mil',stthat, derivamatR, stthat.conj().T)\n",
    "\n",
    "    wsI = 1.0j * jnp.hstack([-jnp.fliplr(myeye), myeye[:,1:]])\n",
    "    wsI = wsI[1:,:]\n",
    "    wsI = wsI.T\n",
    "    ctrmatsI = wsI[toepindxmat]\n",
    "    prederivamatI = jnp.einsum('ij,jkm,kl->ilm',stthat.conj().T, ctrmatsI, stthat)\n",
    "    derivamatI = prederivamatI * jnp.expand_dims(mask, 2)\n",
    "    alldmatimag = -1j * dt * jnp.einsum('ij,jkm,kl->mil',stthat, derivamatI, stthat.conj().T)\n",
    "\n",
    "    alldmat = jnp.vstack([alldmatreal, alldmatimag])\n",
    "\n",
    "    # compute all entries of the gradient at once\n",
    "    gradients = jnp.real(jnp.einsum('bij,ajk,bik->a', jnp.conj(lammatvec[:, 1:]), alldmat, ahatmatvec[:, :-1]))\n",
    "\n",
    "    return gradients\n",
    "\n",
    "\n",
    "# jist adjgrads\n",
    "jitadjgrads = jax.jit(adjgrads)\n",
    "# compile and test jitadjgrads\n",
    "# print(nl.norm(jitadjgrads(thetatrue)))\n",
    "\n",
    "# start optimization (i.e., learning theta)\n",
    "rsltadjthetarnd = so.minimize(jitampsqobject, thetarnd, jac=jitadjgrads, tol=1e-12, options={'maxiter': 1000, 'disp': True, 'gtol': 1e-15}).x\n",
    "\n",
    "# transform learned theta (i.e., vhatmat) to real space potential\n",
    "adjvtoeplearnR = rsltadjthetarnd[:numtoepelms]\n",
    "adjvtoeplearnI = jnp.concatenate((jnp.array([0.0]), rsltadjthetarnd[numtoepelms:]))\n",
    "adjvtoeplearn = adjvtoeplearnR + 1j * adjvtoeplearnI\n",
    "adjvlearnfour = np.sqrt(2 * L) * np.concatenate([np.conjugate(np.flipud(adjvtoeplearn[1:(numfour + 1)])), adjvtoeplearn[:(numfour + 1)]])\n",
    "adjvlearnrec = adjvlearnfour @ fourtox\n",
    "\n",
    "# plot learned potential vs true potential\n",
    "plt.plot(xvec, jnp.real(adjvlearnrec), '.-', label='adj')\n",
    "# plt.plot(xvec, vxvec, label='truth')\n",
    "plt.plot(xvec, jnp.real(vinitrec), label='randtheta')\n",
    "plt.xlabel('x')\n",
    "plt.title('True Potential vs. Learned Potential')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(cwddir / 'graph_true_vs_learned_potential.pdf', format='pdf')\n",
    "\n",
    "# plot shifted learned potential\n",
    "zeroindex = len(xvec) // 2\n",
    "# adjdiff = np.abs(vxvec[zeroindex] - jnp.real(adjvlearnrec)[zeroindex])\n",
    "# plt.plot(xvec, jnp.real(adjvlearnrec) + adjdiff, '.-', label='adj')\n",
    "# plt.plot(xvec, vxvec, label='truth')\n",
    "plt.plot(xvec, jnp.real(vinitrec), label='randtheta')\n",
    "plt.xlabel('x')\n",
    "plt.title('True Potential vs. Shifted Learned Potential')\n",
    "plt.legend()\n",
    "# plt.show()\n",
    "plt.savefig(cwddir / 'graph_shifted_true_vs_learned_potential.pdf', format='pdf')\n",
    "\n",
    "# print('l2 error of shifted adj potential:', nl.norm(jnp.real(adjvlearnrec) + adjdiff - vxvec), sep='\\n')\n",
    "# print('l2 error of shifted and trimmed adj potential:', nl.norm(jnp.real(adjvlearnrec)[125:-125] + adjdiff - vxvec[125:-125]), sep='\\n')\n",
    "# print('l-inf error of shifted and trimmed adj potential:', np.mean(np.abs(jnp.real(adjvlearnrec)[125:-125] + adjdiff - vxvec[125:-125])), sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}